{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0e0812-b4e3-445c-a8ee-eb7d25b12ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjanajd/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/sanjanajd/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "from datasets import load_dataset,load_metric\n",
    "# from evaluate import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import emoji\n",
    "emoji_list = emoji.EMOJI_DATA.keys()\n",
    "emoji_descriptions = [emoji.demojize(e, delimiters=(\"<\", \">\")) for e in emoji_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f6d513-4175-4c17-878d-6b91178cd1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/sanjanajd/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 123.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the Samsum dataset\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"] \n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4ff6292-b0ac-44a7-b255-b15c733e8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize(tokenizer,model,text):\n",
    "#     inputs = tokenizer(f\"Summarize dialogue >>\\n {text}\", return_tensors=\"pt\", max_length=1000, truncation=True, padding=\"max_length\").to(device)\n",
    "\n",
    "#     # Generate summary\n",
    "#     summary_ids = model.generate(inputs.input_ids, num_beams=4, max_length=100, early_stopping=True)\n",
    "# #     print(len(summary_ids[0]))\n",
    "#     # Decode the summary\n",
    "#     summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "#     return summary\n",
    "def summarize(tokenizer,model,text):\n",
    "#     print(f\"Summarize dialogue >>\\n {emoji.demojize(text, delimiters=('<', '>'))}\")\n",
    "    inputs = tokenizer(f\"Summarize dialogue >>\\n {emoji.demojize(text, delimiters=('<', '>'))}\", return_tensors=\"pt\", max_length=1000, truncation=True, padding=\"max_length\").to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs = inputs.input_ids, num_beams=4, max_length=100, early_stopping=True)\n",
    "    # Decode the summary\n",
    "    summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "generated_summaries = []\n",
    "actual_summaries = []\n",
    "generated_summary_orignal = []\n",
    "dialogue_list = []\n",
    "#SAVED_MODEL_PATH = '/Users/sanjanajd/Desktop/Sanjana/NLP/Project/Bart_base_emoji/bart_base_full_finetune_emoji_save'\n",
    "SAVED_TOK_PATH = '/Users/sanjanajd/Desktop/Sanjana/NLP/Project/flant5-base-lora/tokenizer-emoji_t5'\n",
    "SAVED_MODEL_TOK = AutoTokenizer.from_pretrained(SAVED_TOK_PATH)#.to(device)\n",
    "#SAVED_MODEL = AutoModelForSeq2SeqLM.from_pretrained(SAVED_MODEL_PATH).to(device)\n",
    "\n",
    "\n",
    "\n",
    "peft_model_id = \"/Users/sanjanajd/Desktop/Sanjana/NLP/Project/flant5-base-lora/flan_t5_base_lora_finetune_emoji_save_adapter\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)#.to(device)\n",
    "combined_model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path).to(device)\n",
    "combined_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "combined_model = PeftModel.from_pretrained(combined_model, peft_model_id).to(device)\n",
    "combined_model.resize_token_embeddings(len(SAVED_MODEL_TOK))\n",
    "\n",
    "\n",
    "load_16_bit=True\n",
    "model_name = \"google/flan-t5-base\"\n",
    "if load_16_bit:\n",
    "    orignal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name,torch_dtype=torch.float16).to(device)\n",
    "else:\n",
    "    orignal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "orignal_model_tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"\"\"Sarah: Good morning, everyone. Let's dive straight into today's agenda. \n",
    "First up, progress on the marketing campaign. Dan, could you give us an update?\n",
    "Dan: Sure, Sarah. We've finalized the campaign strategy and are currently in the \n",
    "process of creating the content calendar. We're on track to launch by the end of next month.\n",
    "Sarah: Great to hear. Any roadblocks or concerns from your end?\n",
    "Dan: Not at the moment. We're working closely with the creative team to ensure everything \n",
    "aligns with our objectives.\n",
    "Sarah: Excellent. Next item on the agenda, the upcoming product launch. James, how are we \n",
    "looking on that front?\n",
    "James: Things are shaping up well. We've conducted beta testing and received positive feedback. \n",
    "The final adjustments are being made, and we're gearing up for a successful launch next week.\n",
    "Sarah: Fantastic news. Let's make sure all departments are aligned for a smooth rollout.\"\"\"\n",
    "\n",
    "####\n",
    "summarize(SAVED_MODEL_TOK,combined_model,text)\n",
    "\n",
    "model_name = \"facebook/bart-base\"\n",
    "orignal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "orignal_model_tok = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a7ad038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sarah and Dan are discussing progress on the marketing campaign. Dan has finalized the campaign strategy and is in the process of creating the content calendar. The next item is the product launch. James has conducted beta testing and received positive feedback.']\n"
     ]
    }
   ],
   "source": [
    "print(summarize(SAVED_MODEL_TOK,combined_model,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d60d1ddc-4b98-4f33-94e3-29ae3715c421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sarah and Dan are working closely with the creative team on the marketing campaign. They are on track to launch by the end of next month. James is conducting beta testing and received positive feedback. ']\n",
      "[\"Summarize dialogue >> Sarah: Good morning, everyone. Let's dive straight into today's agenda. __________________________________________________________First up, progress on the marketing campaign. Dan, could you give us an update? __________________________________________Dan: Sure, Sarah. We've finalized the campaign strategy and are currently in the __________________________________________________process of creating the content calendar. We're on track to launch by the end of next month. __________Sarah: Great to hear.\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Sarah: Good morning, everyone. Let's dive straight into today's agenda. \n",
    "First up, progress on the marketing campaign. Dan, could you give us an update?\n",
    "Dan: Sure, Sarah. We've finalized the campaign strategy and are currently in the \n",
    "process of creating the content calendar. We're on track to launch by the end of next month.\n",
    "Sarah: Great to hear. Any roadblocks or concerns from your end?\n",
    "Dan: Not at the moment. We're working closely with the creative team to ensure everything \n",
    "aligns with our objectives.\n",
    "Sarah: Excellent. Next item on the agenda, the upcoming product launch. James, how are we \n",
    "looking on that front?\n",
    "James: Things are shaping up well. We've conducted beta testing and received positive feedback. \n",
    "The final adjustments are being made, and we're gearing up for a successful launch next week.\n",
    "Sarah: Fantastic news. Let's make sure all departments are aligned for a smooth rollout.\"\"\"\n",
    "print(summarize(SAVED_MODEL_TOK,SAVED_MODEL,text))\n",
    "print(summarize(orignal_model_tok,orignal_model,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ce72c8-56e2-4b67-9b13-910049ac55a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Emily, Tom and Mike are reviewing the progress on the client project. The client has been positive so far. Lisa will be analyzing the financial data from the previous quarter and will send a proposed budget plan by the end of the day to everyone's review. \"]\n",
      "[\"Summarize dialogue >> _______________________________________________ _______________________________________________________________Emily: Good morning, team. Let's start by reviewing the progress on the client project. _______________________________________________________Tom, could you give us an update? _____________________________________________________________________________________________Tom: Certainly, Emily. We've completed the initial research phase and are now moving into the design stage. ___________________________________________________________________________________________________________________________Feedback from the client has been positive so far, and we're confident in meeting the project deadline\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Emily: Good morning, team. Let's start by reviewing the progress on the client project. \n",
    "Tom, could you give us an update?\n",
    "Tom: Certainly, Emily. We've completed the initial research phase and are now moving into the design stage. \n",
    "Feedback from the client has been positive so far, and we're confident in meeting the project deadline.\n",
    "Emily: That's great to hear, Tom. Keep up the good work. Next on the agenda, budget allocation for the \n",
    "upcoming quarter. Lisa, what's the status on that?\n",
    "Lisa: We've analyzed the financial data from the previous quarter and have identified areas for optimization. \n",
    "I'll be circulating a proposed budget plan by the end of the day for everyone's review.\n",
    "Emily: Excellent. Let's ensure we allocate resources wisely to maximize our ROI. Moving on, I'd like \n",
    "to discuss the recent market trends. Mike, any insights to share?\n",
    "Mike: Yes, Emily. Our market analysis indicates a shift in consumer preferences towards sustainable \n",
    "products. I suggest we explore opportunities to integrate eco-friendly initiatives into our product \n",
    "line to stay ahead of the curve.\n",
    "Emily: Agreed, Mike. Let's prioritize sustainability in our future endeavors. Before we adjourn, \n",
    "does anyone have any urgent matters to address?\"\"\"\n",
    "print(summarize(SAVED_MODEL_TOK,SAVED_MODEL,text))\n",
    "print(summarize(orignal_model_tok,orignal_model,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff863536-6fb6-4005-b540-2526878566bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples summarized:10\ttime:10.299947261810303\n",
      "samples summarized:20\ttime:20.773779153823853\n",
      "samples summarized:30\ttime:31.32620930671692\n",
      "samples summarized:40\ttime:40.521833419799805\n",
      "samples summarized:50\ttime:49.939515829086304\n",
      "samples summarized:60\ttime:59.53868269920349\n",
      "samples summarized:70\ttime:68.67994832992554\n",
      "samples summarized:80\ttime:78.37402582168579\n",
      "samples summarized:90\ttime:88.44464993476868\n",
      "samples summarized:100\ttime:98.30354690551758\n",
      "samples summarized:110\ttime:106.31132197380066\n",
      "samples summarized:120\ttime:116.96791076660156\n",
      "samples summarized:130\ttime:126.02568197250366\n",
      "samples summarized:140\ttime:136.3091561794281\n",
      "samples summarized:150\ttime:145.3128833770752\n",
      "samples summarized:160\ttime:154.7295835018158\n",
      "samples summarized:170\ttime:164.3201379776001\n",
      "samples summarized:180\ttime:173.97152996063232\n",
      "samples summarized:190\ttime:183.81937289237976\n",
      "samples summarized:200\ttime:194.03785586357117\n",
      "samples summarized:210\ttime:204.5013666152954\n",
      "samples summarized:220\ttime:214.65974378585815\n",
      "samples summarized:230\ttime:224.8179349899292\n",
      "samples summarized:240\ttime:233.76940631866455\n",
      "samples summarized:250\ttime:243.55992722511292\n",
      "samples summarized:260\ttime:253.27950739860535\n",
      "samples summarized:270\ttime:262.8040089607239\n",
      "samples summarized:280\ttime:272.41978120803833\n",
      "samples summarized:290\ttime:282.8451192378998\n",
      "samples summarized:300\ttime:292.22990131378174\n",
      "samples summarized:310\ttime:302.68083667755127\n",
      "samples summarized:320\ttime:313.3559970855713\n",
      "samples summarized:330\ttime:322.19229316711426\n",
      "samples summarized:340\ttime:332.47151017189026\n",
      "samples summarized:350\ttime:341.0127503871918\n",
      "samples summarized:360\ttime:351.4823205471039\n",
      "samples summarized:370\ttime:361.1808993816376\n",
      "samples summarized:380\ttime:371.9095149040222\n",
      "samples summarized:390\ttime:381.2041711807251\n",
      "samples summarized:400\ttime:391.9586327075958\n",
      "samples summarized:410\ttime:401.7597484588623\n",
      "samples summarized:420\ttime:411.59758162498474\n",
      "samples summarized:430\ttime:422.42506217956543\n",
      "samples summarized:440\ttime:433.00094056129456\n",
      "samples summarized:450\ttime:442.30031418800354\n",
      "samples summarized:460\ttime:452.194433927536\n",
      "samples summarized:470\ttime:462.32777762413025\n",
      "samples summarized:480\ttime:472.4913229942322\n",
      "samples summarized:490\ttime:481.73141527175903\n",
      "samples summarized:500\ttime:491.76432967185974\n",
      "samples summarized:510\ttime:501.8127956390381\n",
      "samples summarized:520\ttime:510.40531635284424\n",
      "samples summarized:530\ttime:519.6379158496857\n",
      "samples summarized:540\ttime:529.0230519771576\n",
      "samples summarized:550\ttime:538.5667016506195\n",
      "samples summarized:560\ttime:548.8006064891815\n",
      "samples summarized:570\ttime:557.9289758205414\n",
      "samples summarized:580\ttime:568.1100344657898\n",
      "samples summarized:590\ttime:577.4481840133667\n",
      "samples summarized:600\ttime:587.3288021087646\n",
      "samples summarized:610\ttime:597.0412018299103\n",
      "samples summarized:620\ttime:607.7191438674927\n",
      "samples summarized:630\ttime:618.8458468914032\n",
      "samples summarized:640\ttime:627.9254477024078\n",
      "samples summarized:650\ttime:638.2008457183838\n",
      "samples summarized:660\ttime:647.6871168613434\n",
      "samples summarized:670\ttime:656.6400265693665\n",
      "samples summarized:680\ttime:666.919924736023\n",
      "samples summarized:690\ttime:676.6231360435486\n",
      "samples summarized:700\ttime:686.456613779068\n",
      "samples summarized:710\ttime:697.147805929184\n",
      "samples summarized:720\ttime:706.3710644245148\n",
      "samples summarized:730\ttime:716.7632205486298\n",
      "samples summarized:740\ttime:726.9189274311066\n",
      "samples summarized:750\ttime:736.5683045387268\n",
      "samples summarized:760\ttime:746.3559799194336\n",
      "samples summarized:770\ttime:756.5685303211212\n",
      "samples summarized:780\ttime:765.9051263332367\n",
      "samples summarized:790\ttime:775.729250907898\n",
      "samples summarized:800\ttime:785.5480544567108\n",
      "samples summarized:810\ttime:794.6047699451447\n",
      "Total time taken: 804.4386882781982 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "i=1\n",
    "j=0\n",
    "for example in test_data:\n",
    "    if i%10==0:\n",
    "#         print()\n",
    "        j+=10\n",
    "        print(f\"samples summarized:{j}\\ttime:{time.time()-start_time}\")\n",
    "#         print(f\"\",)\n",
    "#     print(example['dialogue'])\n",
    "    generated_summary = summarize(SAVED_MODEL_TOK,SAVED_MODEL,example['dialogue'])\n",
    "    generated_summaries.append(generated_summary[0])\n",
    "    generated_summary_o = summarize(orignal_model_tok,orignal_model,example['dialogue'])\n",
    "    generated_summary_orignal.append(generated_summary_o[0])\n",
    "    actual_summaries.append(example[\"summary\"])\n",
    "    dialogue_list.append(example['dialogue'])\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de025dd0-97ff-4900-8c9f-205ba5802ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mark spent the weekend hiking and watching \"The Crown\" on Netflix. He watched a couple of episodes and liked the portrayal of the royal family\\'s dynamics. He might go camping next weekend if the weather is nice.'],\n",
       " ['Summarize dialogue >> Sarah: Hey Mark, how was your weekend? What was your favorite part of the weekend?Mark: It was great, thanks for asking. I went hiking with some friends. How about you? What were your favorite parts of the day?Sarah: Mine was relaxing. I stayed home and binge-watched a new series on Netflix. It\\'s called \"The Crown.\"Mark: Nice! Which one? What\\'s your favorite thing about it?Sarah'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Sarah: Hey Mark, how was your weekend?\n",
    "\n",
    "Mark: It was great, thanks for asking. I went hiking with some friends. How about you?\n",
    "\n",
    "Sarah: Mine was relaxing. I stayed home and binge-watched a new series on Netflix.\n",
    "\n",
    "Mark: Nice! Which one?\n",
    "\n",
    "Sarah: \"The Crown.\" Have you seen it?\n",
    "\n",
    "Mark: Yeah, I watched a couple of episodes. It's pretty good, but I couldn't get into it as much as I thought I would.\n",
    "\n",
    "Sarah: Really? I find it fascinating, especially the portrayal of the royal family's dynamics.\n",
    "\n",
    "Mark: Yeah, that part is interesting. I guess I prefer something with more action.\n",
    "\n",
    "Sarah: Fair enough. Different strokes for different folks, right?\n",
    "\n",
    "Mark: Exactly. So, any plans for next weekend?\n",
    "\n",
    "Sarah: Not really, just catching up on some reading. How about you?\n",
    "\n",
    "Mark: I might go camping if the weather's nice. It's been a while since I've been out in nature.\n",
    "\n",
    "Sarah: That sounds like fun. Hopefully, the weather cooperates.\n",
    "\n",
    "Mark: Yeah, fingers crossed.\"\"\"\n",
    "summarize(SAVED_MODEL_TOK,SAVED_MODEL,text),summarize(orignal_model_tok,orignal_model,text),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd982f96-5810-4ed4-a12c-96046efc8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "temp_df = pd.DataFrame({'finetune_summary':generated_summaries,'original_summary':generated_summary_orignal,'human_summary':actual_summaries,'dialogue':dialogue_list})\n",
    "temp_df.to_csv(\"results_bart_base_fullfinetune_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ba99795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import emoji\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loading the dataset and tokenizer\n",
    "\n",
    "SAVED_TOK_PATH = '/Users/sanjanajd/Desktop/Sanjana/NLP/Project/flant5-base-lora/tokenizer-emoji_t5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVED_TOK_PATH)\n",
    "\n",
    "# Configuring and loading the PEFT model\n",
    "peft_model_id = \"/Users/sanjanajd/Desktop/Sanjana/NLP/Project/flant5-base-lora/flan_t5_base_lora_finetune_emoji_save_adapter\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path).to(device)\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def summarize(tokenizer, model, text):\n",
    "    text = emoji.demojize(text, delimiters=('<', '>'))\n",
    "    inputs = tokenizer(f\"Summarize dialogue >>\\n {text}\", return_tensors=\"pt\", max_length=1000, truncation=True, padding=\"max_length\").to(device)\n",
    "    summary_ids = model.generate(inputs=inputs.input_ids, num_beams=4, max_length=100, early_stopping=True)\n",
    "    summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    return summary[0]\n",
    "\n",
    "def summarize_dialogue(text):\n",
    "    return summarize(tokenizer, model, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c534439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running on public URL: https://1f55ac58e23d9301ce.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1f55ac58e23d9301ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=summarize_dialogue,\n",
    "    inputs=gr.Textbox(lines=10, placeholder=\"Enter dialogue here...\", label=\"Input Dialogue\"),\n",
    "    title=\"Flan t5 fine tuned using LoRA on SAMsum dataset\",\n",
    "    outputs=gr.Textbox(label=\"Generated Summary\")\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131255b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
